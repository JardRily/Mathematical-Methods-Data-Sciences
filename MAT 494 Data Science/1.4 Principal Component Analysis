{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/1.4%20Principal%20Component%20Analysis\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUiGM0xbonRH"
      },
      "source": [
        "# 1.4 Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyPAeyLeonRJ"
      },
      "source": [
        "PCA is a lot like data compression. Here we are compressing a vector space of higher dimensions into a lower one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqAJURjQonRJ"
      },
      "source": [
        "## 1.4.1 Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEy76xhwonRK"
      },
      "source": [
        "Suppose we have an $m \\times n$ matrix $A$. Then $A^TA$ is symmertric, and therefore can be orthogonally diagonlized. Let $v_1, \\ldots, v_n$ be an orthogonal basis for $\\mathbb{R}^n$ consisting of the eigenvectors of $A^TA$, and let $\\lambda_1, \\ldots, \\lambda_n$ be the associated eigenvalues of $A^TA$. Then, for $1 \\le i \\le n$, $||Av_i||^2 = \\lambda_i$. The eigenvalues of $A$ are nonnegative. \n",
        "\n",
        "We can reorder the eigenvalues so they are arranged in decreasing order: $\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_n$. The singular values of $A$ are the square roots of the eigenvalues of $A^TA$, senoted by $\\sigma_1, \\ldots, \\sigma_n$, and are also arranged in decreasing order so that $\\sigma_i = \\sqrt{\\lambda_i}$ for $1 \\le i \\le n$. The singular values of A are the lengths of the vectors $Av_1, \\ldots, Av_n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmtOYaX4onRK"
      },
      "source": [
        "If an $m \\times n$ martix has $r$ nonzero singular values, $\\sigma_1, \\ldots, \\sigma_r, \\ge 0$ with $\\sigma_{r+1} + \\ldots + \\sigma_n = 0$, then the dimension of $col(A) = r$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVHhIXgconRK"
      },
      "source": [
        "Let $A$ be an $m \\times n$ matrix with the dimension $col(A) = r$. Then there exists a matrix $\\Sigma$ such that the diagonal entries $D$ are the first $r$ singular values of A, where $\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\sigma_r \\ge 0$. There exists an $m \\times m$ orthogonal matrix $U$ and an $n \\times n$ orthogonal matrix $V$ such that: $A = U\\Sigma V^T$.\n",
        "\n",
        "Any factorization of $A = U\\Sigma V^T$ is called a singular value decomposition SVD of $A$. The matrices $U$ and $V$ are nonunique, but $\\Sigma$ is the singular values of $A$. The columns of $U$ in SVD are reffered to as the left singular vectors of $A$, and the columns of $V$ are reffered to as the right singular vectors of A.\n",
        "\n",
        "NOTE: Since $U$ and $V^T$ are both orthoganal, it holds that $U*U = I$ and $V^T*V^T = I$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRELySCuonRL",
        "outputId": "92463e20-76cc-4579-e4c9-0814b70a0210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A:\n",
            " [[11 12 13]\n",
            " [21 22 23]\n",
            " [31 32 33]]\n",
            "u:\n",
            " [[-0.296  0.864  0.408]\n",
            " [-0.541  0.201 -0.816]\n",
            " [-0.787 -0.462  0.408]]\n",
            "s:\n",
            " [70.436  0.852  0.   ]\n",
            "vh:\n",
            " [[-0.554 -0.577 -0.6  ]\n",
            " [-0.726 -0.019  0.688]\n",
            " [ 0.408 -0.816  0.408]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import linalg\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "A = np.array([[11, 12, 13], [21, 22, 23], [31, 32, 33]], dtype=int)\n",
        "u, s, vh = linalg.svd(A)\n",
        "print(f\"A:\\n {A}\")\n",
        "print(f\"u:\\n {u}\")\n",
        "print(f\"s:\\n {s}\")\n",
        "print(f\"vh:\\n {vh}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBbWZZCConRM",
        "outputId": "4f4c3c4c-30f9-4ae2-bb43-40f1db5031a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U:\n",
            " [[-0.296  0.864  0.408]\n",
            " [-0.541  0.201 -0.816]\n",
            " [-0.787 -0.462  0.408]]\n",
            "E:\n",
            " [[70.436  0.     0.   ]\n",
            " [ 0.     0.852  0.   ]\n",
            " [ 0.     0.     0.   ]]\n",
            "V:\n",
            " [[-0.554 -0.726  0.408]\n",
            " [-0.577 -0.019 -0.816]\n",
            " [-0.6    0.688  0.408]]\n"
          ]
        }
      ],
      "source": [
        "# Now to get the matricies in the form UEV\n",
        "U = u\n",
        "E = np.diag((s[0], s[1], s[2]))\n",
        "V = np.transpose(vh)\n",
        "print(f\"U:\\n {U}\")\n",
        "print(f\"E:\\n {E}\")\n",
        "print(f\"V:\\n {V}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlcWN7PionRN",
        "outputId": "66989a52-b34e-4fdb-e690-1fb4dbfb9d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "u1:\n",
            " [-0.296 -0.541 -0.787]\n",
            "u2:\n",
            " [ 0.864  0.201 -0.462]\n",
            "u3:\n",
            " [ 0.408 -0.816  0.408]\n",
            "\n",
            "e1:\n",
            " [70.43631435547283]\n",
            "e2:\n",
            " [0.851833327013623]\n",
            "u3:\n",
            " [9.675393434009421e-17]\n",
            "\n",
            "v1:\n",
            " [-0.554 -0.577 -0.6  ]\n",
            "v2:\n",
            " [-0.726 -0.019  0.688]\n",
            "v3:\n",
            " [ 0.408 -0.816  0.408]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We can also split up the matricies into their respective columns/singular values for later use\n",
        "tempU = np.transpose(U)\n",
        "u1, u2, u3 = tempU[0], tempU[1], tempU[2]\n",
        "e1, e2, e3 = [s[0]], [s[1]], [s[2]]\n",
        "v1, v2, v3 = vh[0], vh[1], vh[2] \n",
        "\n",
        "print(f\"u1:\\n {u1}\\nu2:\\n {u2}\\nu3:\\n {u3}\\n\")\n",
        "print(f\"e1:\\n {e1}\\ne2:\\n {e2}\\nu3:\\n {e3}\\n\")\n",
        "print(f\"v1:\\n {v1}\\nv2:\\n {v2}\\nv3:\\n {v3}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cht-SDcoonRO"
      },
      "source": [
        "## 1.4.2 Low-Rank Matrix Approximations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDsX5tJPonRO"
      },
      "source": [
        "An understanding of norms is important for this section, please reffer to 1.2.2.1 for a definion of norms. \n",
        "\n",
        "Rank is the dimension of the vector space geereated by the columns of $A$, which is found by counting the maximum number of linearly independent vectors in a matrix. A simple way of calculating rank is to change a matrix to RE form and count the number of non-zero rows. A more formal definition of rank is:\n",
        "\n",
        "\\begin{gather*}\n",
        "||A||_2 = \\max_{0 \\neq x \\in \\mathbb{R}^m} \\frac{||Ax||}{||x||} = \\max_{x \\neq 0, ||x|| = 1} ||Ax|| = \\max_{x \\neq 0, ||x|| = 1} x^TA^Tx\n",
        "\\end{gather*}\n",
        "Let $A \\in \\mathbb{R}^{n\\times m}$ be a matrix with SVD\n",
        "\\begin{gather*}\n",
        "A = \\sum_{j=1}^k \\sigma_j u_j v_j^T\n",
        "\\end{gather*}\n",
        "The rank of $A_k$ is $k$. The vectors {$u_j:j=1, \\ldots, k$} are orthonormal, and, since $\\sigma_j > 0$ for $j = 1, \\ldots, k$ and the vectors {$v_j:j=1,\\ldots,k$} are orthonormal, {$u_j:j=1,\\ldots,k$} spans the column space of $A_k$.\n",
        "\n",
        "Since $A=\\sum_{j=1}^r \\sigma_j u_j v_j^T$ we know that $||A-A_k||_2^2 =\\sigma_{k+1}^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1PEYnQionRP"
      },
      "source": [
        "## 1.4.3 Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLsnxdkNonRP"
      },
      "source": [
        "### 1.4.3.1 Covariance Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbiCmxbmonRP"
      },
      "source": [
        "Let $[X_1 \\cdots X_N]$ be a $p \\times N$ matrix of observations. The sample mean $M$ of the observation vectors $X_1 \\cdots X_N$ is given by: $M = \\frac{1}{N}(X_1+\\ldots+X_N)$. The sample mean can be found with $\\hat X_k = X_k - M$. The columns of the $p \\times N$ matrix $B = [\\hat X_1 \\hat X_2 \\cdots \\hat X_N]$ have a zero sample mean, and $B$ is said to be in the mean-deviation form. The sample covariance matrix is the $p \\times p$ matrix $S$ defined by: $S=\\frac{1}{N-1}BB^T$. Since any matrix of the form $BB^T$ is semidefinite, so is $S$. \n",
        "\n",
        "For any symmetric matrix $A$, $A$ is semidefinite if all eigenvalues of $A$ are nonnegative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x61HH-9eonRP"
      },
      "source": [
        "### 1.4.3.2 Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIKEpAXUonRQ"
      },
      "source": [
        "For the following, assume that the $p \\times N$ data matrix $X = [X_1 X_2 \\cdots X_N] is already in mean-deviation form. The goal of PCA is to find $k$ orthonormal vectors $v_1, \\ldots, v_k$ that maximize the objective function: $\\frac{1}{N}\\sum_{i=1}$N \\sum_{j=1}^k \\langle X_i \\cdot v_j \\rangle ^2$, where $k \\le p$, and $XX^T$ is a $p \\times p$ matrix. As a result, for each $j \\le k$, the variance-maximization problem is: $\\argmax_{v:||v||=1} v_j^TXX^Tv_j$. \n",
        "\n",
        "Assume that $XX^T = Vdiag(\\lambda_1, \\ldots, \\lambda_p)V^T$, or $V^TXX^T = (\\lambda_1, \\ldots, \\lambda_p)$.We find that the optimal choice of the first $k$ eigenvectors of $XX^T$ are the first $k$ largest eigenvalues, which are also the first $k$ columns of $V=[v_1 \\ldots v_p]$ of the covariance matrix $XX^T$, which are called the Principal Components of the Data. The First PCD is the eigenvector corresponding to the largest eigenvalue of $XX^T$, the second PCD is the eigenvector corresponding to the second largest eigenvalue, etc. \n",
        "\n",
        "The orthogonal $p\\times p$ matrix $V=[v_1 \\ldots v_p] that determines a change of variable, $x=Vy$, or:\n",
        "\n",
        "\\begin{gather*}\n",
        "\\begin{pmatrix}\n",
        "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p\n",
        "\\end{pmatrix} = \n",
        "\\begin{pmatrix}\n",
        "v_1 & v_2 & \\cdots & v_p\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_p\n",
        "\\end{pmatrix}\n",
        "\\end{gather*}\n",
        "\n",
        "The new variables $y_1, \\ldots y_p$ are uncorelated, and listed in decreasing variance. Therefore:\n",
        "\\begin{gather*}\n",
        "x^TXX^Tx = y^TV^TXX^TVy=y^Tdiag(\\lambda_1, \\ldots, \\lambda_p)y= \\sum_1^p \\lambda_i y_i^2\\\\\n",
        "\\text{NOTE:} \\;\\; y=V^{-1}x = V^Tx\n",
        "\\end{gather*}\n",
        "\n",
        "Let $v_{1i}, \\ldots, v_{pi}$ be the entries in $v_i$. Since $v_i^T$ is the $ith$ row of $V^T$, the equation $y=V^Tx$ shows that: $y_i = v_i^Tx = v_{1i}x_1 + v_{2i}x_2 + \\cdots + v_{pi}x_p$. Thus $y_i$ is a linear combination of the original variables $x_1, \\cdots, x_p$, using the entries the eigenvector $v_i$ as weights, which are called loadings.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsLjeKEBonRQ"
      },
      "source": [
        "### 1.4.3.3 Total Variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrrT3elonRQ"
      },
      "source": [
        "For this we will assum $X$ is already in the mean-deviation form $X=[X_1 X_2 \\cdots X_N]$ and let covariance matrix $S = \\frac{1}{N-1}XX^T$.\n",
        "\n",
        "The diagonal entries $s_{jj}$ in $S$ are called the varaince of $x_j$, which is the first $jth$ row of $X$. The variance of $x_j$ measures the spread of of the values of $x_j$. The total variance of the data is the sum of the variances on the diagonal of $S$. The sum of the diagonal entris of a square matrix $S$ is called the trace of the matrix, written as $tr(S)$ and sometimes also as $trace(S)$. Thus the $Total Variance = tr(S)$, where $tr(S) = \\frac{1}{N-1} \\sum_1^p \\lambda_j$ since $tr(VSV^T) = tr(S)$. The fraction of the first k term truncation is $\\frac{\\Sigma_1^k\\lambda_j}{\\Sigma_1^p\\lambda_j}$."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}