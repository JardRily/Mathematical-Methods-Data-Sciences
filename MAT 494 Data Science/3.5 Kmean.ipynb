{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/3.5%20Kmean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmwA7wPtAsdo"
      },
      "source": [
        "# 3.5 K-means\n",
        "\n",
        "k-means clustering is a popular method of vector quantization that aims to partition $n$ observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. k-means clustering minimizes within cluster variances (squared Eclidean distances), but not regular Euclidean distances. While k-means generally converge quickley to a local optimum, the problem is computationally difficult. \n",
        "\n",
        "Given a $(x_1,x_2,\\ldots, x_n)$ where each observation is a $d$-dimensional real vector, k-means clustering aims to partition the n observations into $k(<n)$ sets $S = \\{S1,...Sk \\}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e.variance), the squared distance of each vector from its centroid summed over all vectors: \n",
        "\\begin{gather*}\n",
        "WCSS_i = \\sum_{x\\in S_i} ||x-\\mu (S_i) ||^2\n",
        "\\end{gather*}\n",
        ", where $\\mu(S_i)$ is the mean of points in $S_i$, $\\mu(S) = \\frac{1}{|S|} \\sum_{x \\in S} x$. The objective is to find $\\arg \\min_S \\sum^k_{i=1} WCSS_i$. K-means Clustering Algorithm: 1) Clusters the data in $k$ groups where $k$ is predfined. 2) Select $k$ points at random as cluster centers. 3) Assign objects to their closest cluster center according to the Euclidean distance function. 4) calculate the centroid or mean of all objects in each cluster. 5) Repeat steps 2, 3, and 4 until the same points are assigned to each cluster in consecutive rounds.\n",
        "\n",
        "We now show that k-means converges by proving that $\\sum^k_{i=1} WCSS_i$ monotonically decreases in each iteration. First, $\\sum^k_{i=1} WCSS_i$ decreases in the reassignment step since each vector is assigned to the closest centroid, so the distance it contributes to $\\sum^k_{i=1}WCSS_i$ decreases. Second, it decreases in the recomputation step because the new centroid is the vector $v$ for which $WCSS_i$ reaches its minimum.\n",
        "\n",
        "\\begin{gather*}\n",
        "WCSS_i(v) = \\sum_{x=(x_j)\\in S} |v-x|^2 = \\sum_{x=(x_j)\\in S} \\sum^d_{j=1}(v_j - x_j)^2 \\\\\n",
        "\\frac{\\partial WCSS_i(v)}{\\partial v_m} = \\sum_{x=(x_j)\\in S_i} 2(v_m - x_m)\n",
        "\\end{gather*}\n",
        "where $x_m$ and $v_m$ are the $m^{th}$ components of their respective vectors. Setting the partial derivative to zero, we get:\n",
        "\\begin{gather*}\n",
        "v=\\frac{1}{|S_i|}\\sum_{x=(x_j)\\in S_i} x_j\n",
        "\\end{gather*}\n",
        "which is the componentwise definition of the centroid. Thus, we minimize $WCSS_i$ when the old centroid is replaced with the new centroid. The sum of the $WCSS_i$ must then also decrease during recomputation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}