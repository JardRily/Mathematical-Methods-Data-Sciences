{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/2.4%20Maximum%20Likelihood%20Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2dpsp0sofd_"
      },
      "source": [
        "# 2.4 Maximum Likelihood Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jg28USaofeB"
      },
      "source": [
        "## 2.4.1 MLE for Random Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fsrptXofeB"
      },
      "source": [
        "MLE is an effective approach of estimating the parameters of a probability distribution through maximizing a likelihood function. The point in the parameter space that maximizes the likelihood function is called the MLE. The logic of maximum likelihood is both intuitive and flexible. As a result, the method has become a dominant means of statistical inference. Let $X_1, X_2, \\ldots ,X_n$ have joint pmf or pdf $f(x_1,x_2,\\ldots,x_n; \\theta_1,\\ldots, \\theta_m)$ where the parameters $\\theta_1, \\ldots, \\theta_m$ have unknown values. When $x_1, \\ldots, x_n$ are the observed sample values and is regarded as a function of $\\theta_1, \\ldots, \\theta_m$, it is called the liklihood function. The MLE $\\hat{\\theta}_1, \\ldots, \\hat{\\theta}_m$ are those values of the $\\theta_i$'s that maximize the likelihood funciton, so that $f(x_1, \\ldots, x_n; \\hat{\\theta}_1, \\ldots, \\hat{\\theta}_m) \\ge f(x_1, \\ldots, x_n; \\theta_1, \\ldots,\\theta_m)$ for all $\\theta_1, \\ldots, \\theta_m$. When the $X_i$'s are substituted in place of the $x_i$'s, the maximum likelihood estimators result. Let $X_1, \\ldots, X_n$ be a random sample from a normal distribution. The likelihood function is \n",
        "\n",
        "\\begin{gather*}\n",
        "f(x_1, \\ldots, x_n; \\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\frac{-(x_1-\\mu)^2}{2\\sigma^2}} \\cdots \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\frac{-(x_n-\\mu)^2}{2\\sigma^2}}=(\\frac{1}{2\\pi \\sigma^2})^\\frac{n}{2} e^{-\\frac{\\Sigma(x_i-\\mu)^2}{2\\sigma^2}} \\\\\n",
        "ln[f(x_1, \\ldots, x_n; \\mu, \\sigma^2)]=-\\frac{n}{2} ln(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2}\\Sigma(x_i-\\mu)^2\n",
        "\\end{gather*}\n",
        "\n",
        "To find the maximizing values of $\\mu$ and $\\sigma^2$, we must take the partial derivatives of $ln(f)$ with respect to $\\mu$ and $\\sigma^2$, equate them to zero, and solve the resulting two equations. First taking derivative with respect to $\\mu$, we have $\\frac{\\partial ln[f(x_1, \\ldots, x_n; \\mu, \\sigma^2)]}{\\partial \\mu}=-\\frac{1}{\\sigma^2}\\Sigma(x_i-\\mu)$. Equating the derivative to zero and solving for $\\mu$ results in the following: $\\hat{\\mu} = \\frac{1}{n}\\Sigma x_i$. Similarly, taking derivative with respect to \n",
        "\n",
        "\\begin{gather*}\n",
        " \\sigma^2, \\frac{\\partial ln[f(x_1, \\ldots, x_n; \\mu, \\sigma^2)]}{\\partial \\mu}=-\\frac{n}{2\\sigma^2}+\\frac{1}{2\\sigma^4}\\Sigma(x_i-\\mu)^2 \\\\\n",
        " \\sigma^2=\\frac{\\Sigma(x_i -\\mu)^2}{n} \\\\\n",
        " \\text{The resulting MLEs:} \\\\\n",
        " \\hat{\\mu}=\\bar{X}, \\hat{\\sigma}^2=\\frac{\\Sigma(X_i-\\bar{X})^2}{n}\n",
        "  \\end{gather*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTHqZHNKofeC"
      },
      "source": [
        "## 2.4.2 Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kOpNN4KofeC"
      },
      "source": [
        "Given input data points $\\{x_i, y_i)\\}^n_{i=1}$, we seek an affine function to fit the data and each $x_i=(x_{i1},\\ldots, x_{ip}). The common approach involves finding coefficients $\\Beta_j, j=1 \\ldots, p$'s that maximize the criterion $\\sum^n_{i=1} (y_i-\\hat{y}_i)^2$, where $\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}$. Now we wish to discuss it from a probabilistic point of view by the maximum likelihoodestimation. consider that we have $n$ points, each of which is drawn in an independent and identically distributed way from the normal distribution. For a given $\\mu, \\sigma^2$, the probability of those $n$ points being drawn define the likelihood function, which are just the multiplication of $n$ normal pdf (because they are independent).\n",
        "\\begin{gather*}\n",
        "\\mathscr{P}(\\mu | y) = \\prod^n_{i=1} P_Y (y_i | \\mu, \\sigma^2) = \\prod^n_{i=1} \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}}\n",
        "\\end{gather*}\n",
        "Since $y$ is a random variable, $y_i = \\hat{y}_i + \\varepsilon$, where $\\varepsilon ~ N(0, \\sigma^2). Thus, $y_i$ is a normal variable with mean as a linear fucniton of $x$ and a fixed standard deviation $y_i ~ N(\\hat{y}_i, \\sigma^2)$. As a result, for each $y_i$, we choose $\\mu$ in the normal disrtibutions as $\\mu_i = \\hat{y}_i$. Hence we derive the maximum likelihood estimate \n",
        "\\begin{gather*}\n",
        "\\hat{\\beta}=argmax \\mathscr{P}(\\beta | y)=argmax_\\beta \\prod^n_{i=1} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(y_i-\\hat{y}_i)^2}{2\\sigma^2}}\\\\\n",
        "= argmin_\\beta \\sum^n_{i=1}(y_i-\\hat{y}_i)^2\n",
        "\\end{gather*}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}