{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/2.3%20Independent%20Variables%20and%20Random%20Samples\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXwbtWlTTcV"
      },
      "source": [
        "# 2.3 Independent Variables and Random Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjwkm4oCTTcW"
      },
      "source": [
        "## 2.3.1 Joint Probability Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQWK-ZjbTTcW"
      },
      "source": [
        "Joint porbability is the probability of two or more related events happening together. The joint probability distribution shows a probability distribution for two or more random variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_s1btZTTcX"
      },
      "source": [
        "### 2.3.1.1 Two Discrete Random Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXujv8QoTTcX"
      },
      "source": [
        "The pmf of a singlediscrete random variable $X$ can be extended to two variables $X, Y$ for describing how much probabiliity mass is placed on each pair of values $(x, y)$. Let $X$ and $Y$ be two discrete random variables defined on the sample space $S$. The joint pmf $p(x, y)$ is defined for each pair of numbers $(x, y)$ by $p(x, y) = P(X=x and Y=y)$. It must be the case that $p(x, y) \\ge 0$ and $\\Sigma_x \\Sigma_y p(x, y) =  1$. \n",
        "\n",
        "The marginal distribution of a subset of a collection of random variables is the ditribution of one variable ith no refference to the other variables. The marginal pmf of $X$, denoted by $p_x(x)$, is given by $p_x(x)=\\Sigma_{y:p(x, y) \\ge 0} p(x, y)$ for each possible value x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwMUnLySTTcX"
      },
      "source": [
        "### 2.3.1.2 Two Continuous Random Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVUFV14ATTcX"
      },
      "source": [
        "The joint continuous distribution is the continuous analogue of a joint discrete distribution. The probability that the pair $(X, Y)$ of a continuous random variable falls into a two dimensional set $A$, which can be obtained by integrating a function called the joint density funciton. Let $X$ and $X$ be continuous random variables. A joint probability density function $f(x, y)$ for these two variables is a function satisfying $f(x, y) \\ge 0$ and $\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty}f(x, y) dx dy = 1$. Then for any two dimensional set $A$, it follows that $P|(X, Y) \\in A| = \\int \\int_A f(x, y) dx dy$.\n",
        "\n",
        "If $f(x, y)$ is a surface at height $f(x, y)$ above the point $(x, y)$ in a three-deimensional coordinate system, then $P|(X, Y) \\in A| is the volume underneath this surface, and above the region $A$. The marginal probability density fucntions of $X$ and $Y$, denoted by $f_X(x)$ and $f_Y(y)$ are given by:\n",
        "\\begin{gather*}\n",
        "f_X(x) = \\int_{-\\infty}^{\\infty} f(x, y)dy for -\\infty < X < \\infty \\\\\n",
        "f_Y(y) = \\int_{-\\infty}^{\\infty} f(x, y) dx for  -\\infty < X < \\infty\n",
        "\\end{gather*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvaR0uLCTTcY"
      },
      "source": [
        "### 2.3.1.3 Independent Random Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrpO_0IATTcY"
      },
      "source": [
        "The concept of independent random variables is very similar to independent events. In most situations, observing the values of $X$ and $Y$ gives information about the value of the other. Indepenedent random variables describe a situation where the occurence of one does not affect the probability of the other, denoted as $P(A \\cap B) = P(A) \\cdot P(B)$. \n",
        "\n",
        "Two random variables $X$ and $Y$ are said to be independent if for every pair of $x$ and $y$ values $p(x, y) = p_X(x) \\cdot p_Y(y)$ when $X$ and $Y$ are discrete, or $f(x, y) = f_X(x) \\cdot p_Y(y)$ when $X$ and $Y$ are continuous. If either is not satisfied for all $(x, y)$, then $X$ and $Y$ are said to be dependent. \n",
        "\n",
        "Suppose that the lifetimes of two components are independent of one another and that the first lifetime, $X_1$, has an exponential distribution with parameter $\\lambda_1$, whereas the second, $X_2$, has an exponential distribution with parameter $\\lambda_2$. Then the joint pdf is \n",
        "\\begin{gather*}\n",
        "f(x_1, x_2) = f_{X_1}(x_1) \\cdot f_{X_2} (x_2) = \n",
        "\\begin{cases}\n",
        "\\lambda_1 e^{-\\lambda_1 x_1 \\cdot \\lambda_2 e^{-\\lambda_2 x_2}} = \\lambda_1 \\lambda_2 e^{-\\lambda_1 x_1 - \\lambda_2 x_2} & x_1 > 0, x_2 > 0 \\\\\n",
        "0 & otherwise\n",
        "\\end{cases}\n",
        "\\end{gather*}\n",
        "\n",
        "Let $\\lambda_1 = \\frac{1}{1100}$ and $\\lambda_2 = \\frac{1}{1300}$, so that the expected lifetimes are $1100\\;hours$ and $1300\\;hours$. The probabiliity that both component lifetimes are at least $1400 \\;hours$ is $P(1400 \\le X_1, 1400 \\le X_2) = P(1400 \\le X_1) \\cdot P(1400 \\le X_2) = e^{\\lambda_1 (1400)} \\cdot e^{-\\lambda_2 (1400)} = (.28)(.34)=.0952$. \n",
        "\n",
        "This applies for more than two variables. If $X_1, X_2, \\ldots, X_n$ are all discrete random variables, the joint pmf of the variables is the function $p(x_1, x_2, \\ldots, x_n)=P(X_1=x_1, X_2=x_2, \\ldots, X_n=x_n)$. If the variables are continuous, the join pdf of $X_1, \\ldots, X_n$ is the funciton $f(x_1, x_2, \\ldots, x_n)$ such that for any $n$ intervals $|a_1, b_1|, \\ldots, |a_n, b_n|$, $P(a_1 \\le X_1 \\le b_1, \\ldots, a_n \\le X_n \\le b_n) = \\int_{a_1}^{b_1} \\cdots \\int_{a_n}^{b_n} f(x_1, \\ldots, x_n)dx_n \\ldots dx_1$.\n",
        "\n",
        "The random variables $X_1, X_2, \\ldots, X_n$ are said to be independent if for every subset of the variables, the join pmf of pdf is equal to the product of the marginal pmfs or pdfs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHW8yVoxTTcY"
      },
      "source": [
        "## 2.3.2 Correlation and Dependence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nf4Jf3cTTcZ"
      },
      "source": [
        "Covariance is a measure of the joint variablility of two random variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJWz0_YjTTcZ"
      },
      "source": [
        "### 2.3.2.1 Correlation for Random Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs-UVkcDTTcZ"
      },
      "source": [
        "When two random variables $X$ and $Y$ are not independent, it is useful to assess how strongly they are related. Let $X$ and $Y$ be jointly distributed random variables with pmf $p(x, y)$ or pdf $f(x, y)$ according to whether the variables are discrete or continuous. The covariance between two random variables $X$ and $Y$ is:\n",
        "\\begin{gather*}\n",
        "Cov(X, Y) = E|(X-\\mu_X)(Y-\\mu_Y)| = \n",
        "\\begin{cases}\n",
        "\\Sigma_x \\Sigma_y (x-\\mu_X) (y-\\mu_Y)p(x, y) & X, Y discrete \\\\\n",
        "\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x-\\mu_X) (y-\\mu_Y)f(x, y) dx dy & X, Y continuous\n",
        "\\end{cases}\n",
        "\\end{gather*}\n",
        "\n",
        "The covariance is the expected product of deviations $Cov(X, X) = E[(X-\\mu_X)^2] = V(X)$. \n",
        "\n",
        "Correlation coefficient is the covariance of the two variables divded by the product of their standard deviations, which is a measure of linear correlation between two variables or sets of data. The correlation ciefficient of $X$ and $Y$, denoted by $Corr(X, Y), \\rho_{X, Y}$ or just $\\rho$, is defined by $\\rho = \\frac{Cov(X, Y)}{\\sigma_X \\cdot \\sigma_Y}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STz5kOpnTTcZ"
      },
      "source": [
        "### 2.3.2.2 Correlation For Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdE4bjgVTTcZ"
      },
      "source": [
        "Correlation coefficients, when applied to a sample, is commonly represented by $r_{xy}$ and may be reffered to as the sampel correlation coefficient. Given paired data ${(x_1, y_1), \\ldots, (x_n, y_n)}$ consisting of $n$ pairs:\n",
        "\n",
        "\\begin{gather*}\n",
        "r_{xy} = \\frac{\\Sigma_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\Sigma_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\Sigma_{i=1}^n (y_i - \\bar{y})^2}} \\\\\n",
        "\\text{where}\\; \\bar{x} = \\frac{1}{n}\\Sigma_{i=1}^n x_i\n",
        "\\end{gather*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAIGU-z5TTca"
      },
      "source": [
        "## 2.3.3 Random Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNNm4onPTTca"
      },
      "source": [
        "### 2.3.3.1 Random Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCPALDdsTTca"
      },
      "source": [
        "A simple random sample is a randomly selected subset of a population and often is used in practice. The random variables $X_1, X_2, \\ldots, X_n$ are said to form a random sample of size $n$ if 1) The $X_i$  are independent random variables, and 2) Every $X_i$ has the same probability distribution. Using the sample mean $\\bar{X} = \\frac{1}{n}(X_1 + \\cdots + X_n)$. Some of the most frequently used inferential procedures are based on properties of the sampling distribution of $\\bar{X}$. We review these relationships between $E(\\bar{X})$ and $\\mu$ and also among $V(\\bar{X}), \\sigma^2$ and $n$.\n",
        "\n",
        "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a distribution with mean value $\\mu$ and standard deviation $\\sigma$. Then 1) $E(\\bar{X}) = \\mu_X = \\mu$, and 2) $V(\\bar{X}) = \\sigma_X^2 = \\sigma^2 / n$ and $\\sigma_X = \\sigma / \\sqrt{n}$. In addition, with $T_0 = X_1 + \\cdots + X_n$ (the sample total), $E(T_0) = n\\mu , V(T_0)=n\\sigma^2 ,$ and $\\sigma_{T_0} = \\sqrt{n}\\sigma$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl9rzh_oTTca"
      },
      "source": [
        "### 2.3.3.2 The Central Limit Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U0t2WCGTTca"
      },
      "source": [
        "The central limit theorem (CLT) indicates that the properly normalized sum of independent random variables tends toward a normal distribution even if the original variables themselves are not normally distributed. Let $X_1, X_2, \\ldots, X_n$ be a random sample from a distribution with mean $\\mu$ and variance $\\sigma^2$. Then if $n$ is sufficiently large, $\\bar{X}$ has approximately a normal distribution with $\\mu_X = \\mu$ and $\\sigma_X^2 = \\sigma^2 / n$ and $T_0$ also has approximately a normal distribution with $\\mu_{T_0} = n\\mu , \\sigma_{T_0}^2 = n\\sigma^2$. The larger the $n$, the better the approximation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}