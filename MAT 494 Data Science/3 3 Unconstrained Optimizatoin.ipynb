{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/3%203%20Unconstrained%20Optimizatoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIHdxEoYoxSS"
      },
      "source": [
        "# 3.3 Unconstrained Optimization\n",
        "\n",
        "## 3.3.1 Necessary and Sufficent Conditions of Local Minimizers\n",
        "\n",
        "We will be looking at unconstrained optimization of the form:\n",
        "\\begin{gather*}\n",
        "\\min_{x\\in\\mathbb{R}^d} f(x)\n",
        "\\end{gather*}\n",
        "where $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$. In this section we are going to use many forms, ideally to find a global minimizer to the optimization problem above.\n",
        "\n",
        "Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R} $. The point $x^* \\in \\mathbb{R}^d $ is a global minimizer of $f$ over $\\mathbb{R}^d$ if $f(x) \\ge f(x^*), \\forall x \\in \\mathbb{R}^d$. Often it is difficult to find a global minimizer unless some special structure is present, which is why we will be introducing weaker notations of solutions. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$. The point $x^* \\in \\mathbb{R}^d$ is a local minimizer of $f$ over $\\mathbb{R}^d$ if there is a $\\delta > 0$ such that: $f(x) \\ge f(x^*), \\forall x \\in B_\\delta (x^*) \\; \\backslash \\; \\{x^*\\}$. If the inequality is strict, we say that $x^*$ is a strict local minimizer.\n",
        "\n",
        "$x^*$ is a local minimizer if there is an \"open ball\" around $x^*$ where it attains the minimum value. We will characterize local minimizers in terms of the gradient and Hessian of the function. We first need to define what a descent direction is, which generalizes the case when the derivative of a one dimensional fucntion is negative. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$. A vector $v$ is a descent direction for $f$ at $x_0$ if there is an $\\alpha^* > 0$ such that $f(x_0 + \\alpha v) < f(x_0), \\forall \\alpha \\in (0, \\alpha^*)$. In the continuously differentiable case, the directional derivative gives a criterion for descent directions. \n",
        "\n",
        "Let $f : \\mathbb{R}^d \\to \\mathbb{R} $ be continuously differentiable at $x_0$. A vector $v$ is a descent direction for $f$ at $x_0$ if $\\partial \\frac{f(x_0)}{\\partial v} = \\nabla f(x_0)^T v < 0$, that is the directional derivative of $f$ at $x_0$ in the direction $v$ is negative. \n",
        "\n",
        "Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable at $x_0$ and assume that $\\nabla f(x_0) \\neq 0$. Then, $f$ has a descent direction at $x_0$.\n",
        "\n",
        "Let $f : \\mathbb{R}^d \\to \\mathbb{R} $ be continuously differentiable on $\\mathbb{R}^d$. If $x_0$ is a local minimizer, then $\\nabla f(x_0) = 0$.\n",
        "\n",
        "A square symmetric $d \\times  d$ matrix $H$ is a positive semi-definite (PSD) if $x^T Hx \\ge 0$ for any $x \\in \\mathbb{R}^d$. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be twice continuously differentiable on $\\mathbb{R}$. If $x_0$ isa local minimizer, then $H_f(x_0)$ is PSD. \n",
        "\n",
        "### 3.3.1.1 Sufficient Conditions for Local Minimizers\n",
        "\n",
        "As in the one dimensional case, the necessary conditions int he previous subsection are not in general sufficient. We can state the following theorum which give a sufficient conditions for local minimizers. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be twice continuously differentiable on $\\mathbb{R}^d$. If $\\nabla f(x_0)=0$ and $H_f (x_0)$ is positive definite, then $x_0$ is a strict local minimizer.\n",
        "\n",
        "## 3.3.2 Convexity and Global Minimizers\n",
        "\n",
        "A real valued function is called convex if the line segment between any two points on the graph of the function lies above the graph between the two points. Our optimality conditions have only concerned local minimizers. Indeed, in the absence of global srtucture, local information such as gradients and Hessians can only inform about the immediate neighborhood of points. Here we consider convexity, under which local minimizers are also global minimizers. \n",
        "\n",
        "### 3.3.2.1 Convex Sets and Functions\n",
        "\n",
        "We start with covex sets. A set $D \\subseteq \\mathbb{R}^d$ is convex if for all $x, y \\in D$ and all $\\alpha \\in [0, 1]$, $(1-\\alpha)x + \\alpha y \\in D$. \n",
        "\n",
        "A function $f : \\mathbb{R}^d \\to \\mathbb{R}$ is convex if for all $x,y \\in \\mathbb{R}^d $ and all $\\alpha \\in [0, 1]$, $f((1 - \\alpha)x + \\alpha y) \\le (1 - \\alpha)f(x) + \\alpha f(y)$. More generally, a funciton $f : D \\to \\mathbb{R}$ over a convex domain $D \\subseteq \\mathbb{R}^d$ is convex if the definition above holds over all $x, y \\in D$.\n",
        "\n",
        "Let $w \\in \\mathbb{R}^d$ and $b\\in \\mathbb{R}$. The function $f(x)=w^Tx+b$ is convex. \n",
        "\n",
        "A common way to prove that a function is convex is to look at its Hessian. We start with a first-order condition that will prove useful. Let $f : \\mathbb{R}^d \\in \\mathbb{R}$ be continuously differentiable. Then $f$ is convex iff for all $x, y\\in \\mathbb{R}^d$, $f(y) \\ge f(x) + \\nabla f(x)^T (y-x)$. \n",
        "\n",
        "Now lets look at a second-order convexity condition. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be twice continuously differentiable. Then $f$ is convex iff, for all $x \\in \\mathbb{R}^d, H_f (x)$ is PSD.\n",
        "\n",
        "We showed in the last section the quadratic $f(x) = \\frac{1}{2}x^T Px + q^T x + r$ has Hessian $H_f (x) = \\frac{1}{2}[P+P^T]$. So $f$ is convex iff the matrix $\\frac{1}{2}[P+P^T]$ has only nonnegative eigenvalues.\n",
        "\n",
        "### 3.3.2.2 Global Minimizers of Convex Functions\n",
        "\n",
        "For a convex function, sufficient condition for minimizer is $\\nabla f (x_0) = 0$. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be a continuously differentiable, convex function. If $\\nabla f(x_0) =0$ then $x_0$ is a global minimizer. \n",
        "\n",
        "Consider the quadratic $f(x) = \\frac{1}{2} x^T Px + q^T x + r$, where $P$ is symmetric and positive definite. The Hessian is then $H_f (x) = \\frac{1}{2} [P+P^T] = P$ for any $x$. So $f$ is convex. Further the gradient is $\\nabla f(x) = Px + q$ for all $x$. Any $x$ satisfying $Px + q=0$ is a global minimizer. If $P= Q \\Lambda Q^T$ is a spectral decomposition of $P$, where all diagonal entries of $\\Lambda$ are strictly positive, then $P^{-1} = Q \\Lambda^{-1} Q^T$ where the diagonal entries of $\\Lambda^{-1}$ are the inverse of those of $\\Lambda$. That can be seen by checking that $Q \\Lambda Q^T Q \\Lambda^{-1} Q^T = QI_{d \\times d} Q^T = I_{d \\times d}$. So the following is a global minimizer $x^* = -Q \\Lambda^{-1} Q^T q$.\n",
        "\n",
        "## 3.3.3 Gradient Descent\n",
        "\n",
        "Gradiant descent is an iterative optimization algorithm for finding local minimum of a differentiable function. Once we know a function has a minimizer, we will discuss a class of algorithms known as gradient descent method for solving optimization problems numerically. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ is continuously differentiable. We restrict ourselves to unconstrained minimization problems of the form $\\min_{x\\in \\mathbb{R}^d} f(x)$. One method is to evaluate $f$ at a large number of points $x$ to identify the global minimizer. A more resource efficient method is to find all staionary points of $f$, that is, those $x$ such that $\\nabla f(x) = 0$. Then choose that $x$ among them that produces the smallest value of $f(x)$. \n",
        "\n",
        "One example of this solution is shown in this least-squares problem $\\min_{x\\in\\mathbb{R}^d} ||Ax-b||^2$, where $A\\in \\mathbb{R}^{d \\times d}$ has full column rank; in particular, $d \\le n$. The objective function is a quadratic function $f(x) = ||Ax - b||^2 = (Ax-b)^T (Ax-b)=x^T A^T Ax - 2b^T Ax + b^T b$. By a previous example, $\\nabla f(x) = 2A^T Ax - 2A^T b$, where we used that $A^T A$ is syymetric. Therefore, the stationary points satisfy $A^T Ax = A^T b$, which can be recognized as the normal equations for the least-squares problem. We have shown in a previous section that when $A$ has full column rank, there is a unique solution to this system. Moreover, this optimization problem is convex. The Hessian of $f$ is $H_f (x) = 2A^T A$. This Hessian is clearly PSD since for any $z \\in \\mathbb{R}^d$, $\\langle z, 2A^T Az \\rangle = 2(Az)^T(Az) = 2||Az||^2 \\ge 0$.\n",
        "\n",
        "Any local minimizer, which is necessarily a stationary point, is also a global minimizer. So we have found all global minimizers. In general, identifying stationary points often leads to systems of nonlinear equations that do not have explicit solutions. Hence, we resort to gradient descent method.\n",
        "\n",
        "### 3.3.3.1 Steepest Descent\n",
        "\n",
        "The steepest descent approach is to find smaller values of $f$ by successively following directions in which $f$ decreases. As we have seen, $-\\nabla f$ provides such a direction. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable at $x_0$. For any unit vector $v \\in \\mathbb{R}^d$, $\\frac{\\partial f(x_0)}{\\partial v} \\ge \\frac{\\partial f(x_0)}{\\partial v^*}$, where $v^* = -\\frac{\\nabla f(x_0)}{||\\nabla f(x_0)||}$.\n",
        "\n",
        "At each iteration of the steepest descent, we take a step in the direction of the negative of the gradient, that is, $x^{k+1} = x^k - \\alpha_k \\nabla f(x^k), k=0,1,2...$ for a sequence of steplengths $\\alpha_k > 0$. $\\alpha_k$ are called step sizes. In general, we will not be able to guarentee that a global minimizer is reached in the limit, even if one exists. \n",
        "\n",
        "Suppose that $f : \\mathbb{R}^d \\to \\mathbb{R}$ is twice continuously differentiable. The step size is chosen to minimize $\\alpha_k=argmin_{\\alpha>0} f(x^k - \\alpha \\nabla f(x^k))$. Then steepest descent started from any $x^0$ produces a sequence $x^k, k=1,2,\\ldots$ such that if $\\nabla f(x^k) \\neq 0$, then $f(x^{k+1}) \\le f(x^k), \\forall k \\ge 1$."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}