{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/3.6%20Support%20Vector%20Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZBpb-NT_TPj"
      },
      "source": [
        "# 3.6 Suport Vector Machine\n",
        "\n",
        "Support-vector machines (SVM) are supervised learning models in machine learning, which aim to analyze data for classification and regression analysis. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other. The objective of the support vector machine algorithm is to find a hyperplane ina  high dimensional space of the number of features that distinctly classifies the data points. An SVM maps training examples to points in space so as to maximize the width of the gap between the two categories. Predictions of new data are based on which side of the gap they fall.\n",
        "\n",
        "We are given a training dataset of $n$ points of the form $(x_1, y_1), \\ldots ,(x_n, y_n)$, where the $y_i$ are either 1 or -1 (each indicating the class to which the point $x_i$ belongs). Each $x_i$ is a $p$-dimensional real vector. We want to maximize the margin distance of hyperplanes that divides the group of points $x_i$ for which $y_i=1$ from the group of points for which $y_i=-1$. Maximizing the margins distance provides some reinforcement so that future data points can be classified with more confidence. \n",
        "\n",
        "A hyperplane can be written as the set of points $x$ satisfying $w^T x-b=0$, where $w$ is the normal vector to the hyperplane. If the training data is linearly seperable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called hte \"margin\". The maximum-margin hyperplane is the hyperplane that lies halfway between them. \n",
        "\n",
        "We are interested in two regions: anything on or above this boundary is of one class, with label 1, and anything on or below this boundary is of the other class, with label -1. The two hyperplanes can be respectivelt described by $w^T x -b=1$, and $w^T x - b=-1$. We wish all data points to fall into the margin, which can be expressed as for each $i$ either $w^T x_i - b \\ge 1$, if $y_i=1$, or $w^T x_i - b \\le -1$, if $y_i=-1$. Together the two constraints that each data point must lie on the correct side of the margin, can be rewritten as $y_i(w^T x_i - b) \\ge 1$ for all $1 \\le i \\le n$. We can put this together to get the optimization probelm.\n",
        "\n",
        "The goal of the optimization is to minimize \n",
        "\\begin{gather*}\n",
        "\\min_{w,b} \\langle \\lambda ||w||^2 + \\frac{1}{n} \\sum^n_{i=1} \\max \\{0, 1-y_i(\\langle w,x_i \\rangle - b)\\} \\rangle\n",
        "\\end{gather*}\n",
        "where the first term $\\lambda ||w||^2$ is the regularizer, and sum is the error term. This minimizes $||w||$ subject to $y_i(w^T x_i - b) \\ge 1$, for all $1 \\le i \\le n$. The first term above is called the regularization term which arises directly from the margin. The parameter $\\lambda$ adjusts the trade-off between increasing the margin size and ensuring that $x_i$ lie on the correct side of the margin while we choose the distance of two hyperplanes to be $\\frac{2}{||w||}$. \n",
        "\n",
        "In principle, the unconstrained optimization problem can be directly solved with gradient descent methods. Because this function is convex in the $w$ we can easily apply a gradient descent method to find the minimum. For example, with stochastic gradient descent pick an $i$ at random and update according to \n",
        "\\begin{gather*}\n",
        "b_{new} = b_{old} - \\beta \n",
        "\\begin{cases}\n",
        "y_i & if 1-y_i(w^T x_i - b)>0 \\\\\n",
        "0 & otherwise\n",
        "\\end{cases}\n",
        "\\end{gather*}\n",
        "and \n",
        "\\begin{gather*}\n",
        "w_{new} = w_{old} - \\beta \n",
        "\\begin{cases}\n",
        "2\\lambda w-\\frac{1}{n}y_i x_i & if 1-y_i (w^T x_i - b) >0 \\\\\n",
        "2\\lambda w & otherwise\n",
        "\\end{cases}\n",
        "\\end{gather*}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}