{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR Decomposition can be used to solve the linear least squares problem and is the basis for the QR algorithm. The $Q$ is an orthogonal matrix, and $R$ is an upper trianglar matrix:\n",
    "\n",
    "\\begin{align}\n",
    "QQ^T = I \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; Q^T = Q^{-1} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n",
    "0 & a_{22} & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & a_{nn}\\\\\n",
    "\\end{bmatrix}\\;\\;\\;\\;\\;\\\\ \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR Decomposition follows the form $A = QR$, and in some cases is easier seen in the form $A^T = Q^T R^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Least-Squares Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{R}^{n*m}$ be an $n*m$ matrix, and $b \\in \\mathbb{R}^n$ be a vector. The system $Ax=b$ is consistant if there exists a set $K$ of n-tuple solutions $s$, such that $s = (s_1, \\ldots, s_n) \\in \\mathbb{R}^n$ satisfying $As = b$. If the solution set $K$ is empty, the system is inconsistant (i.e. if $K \\neq \\emptyset$, the system is consistent, if $K = \\emptyset$ the system is inconsistent). When $n=m$, we can use the inverse $A^{-1}$ to solve the system; however, when $n > m$, $A^{-1}$ does not exist. We can still solve this by finding the solution to the following least squares problem:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\min_{x \\in \\mathbb{R}^m} || Ax-b|| \\\\\n",
    "\\text{The solution to this satisfies:}\n",
    "\\\\\n",
    "A^TAx=A^Tb \\text{ and } Rx^*=Q^Tb, \\text{ where } x^* \\text{ is the solution that can be found using back substitution.}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of input data points $\\{(x_i,y_i)\\}^n_{i=1}$, where $x_i=(x_{i1},\\ldots,x_{id})^T$ is a column vector, we can find an affine function (a function thats composed of a linear function plus a constant) to fit the data (i.e. an affine function that is the line of best fit). This can be done by finding some coefficients $\\beta_j$ that minimizes the following:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\sum^n_{i=1}(y_i-\\hat{y_i})^2 \\\\\n",
    "\\hat{y_i} = \\beta_0+\\sum^d_{j=1}\\beta_jx_{ij}\n",
    "\\end{gather*}\n",
    "\n",
    "This can be viewed as the predicted values of the linear model with coefficients $\\beta_j$. The problem can be transformed to the following least-squares problem:\n",
    "\\begin{gather*}\n",
    "\\min_\\beta ||y-A\\beta||^2\n",
    "\\end{gather*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
