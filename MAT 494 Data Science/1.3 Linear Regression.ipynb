{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR Decomposition can be used to solve the linear least squares problem and is the basis for the QR algorithm. The $Q$ is an orthogonal matrix, and $R$ is an upper trianglar matrix:\n",
    "\n",
    "\\begin{align}\n",
    "QQ^T = I \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; Q^T = Q^{-1} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n",
    "0 & a_{22} & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & a_{nn}\\\\\n",
    "\\end{bmatrix}\\;\\;\\;\\;\\;\\\\ \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR Decomposition follows the form $A = QR$, and in some cases is easier seen in the form $A^T = Q^T R^T$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\MAT 494 Data Science\\1.3 Linear Regression.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/MAT%20494%20Data%20Science/1.3%20Linear%20Regression.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We're gonna start using the SciPy library to do some more complex matrix opperations\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/MAT%20494%20Data%20Science/1.3%20Linear%20Regression.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/MAT%20494%20Data%20Science/1.3%20Linear%20Regression.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/MAT%20494%20Data%20Science/1.3%20Linear%20Regression.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Lets start with our matrix A\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "# We're gonna start using the SciPy library to do some more complex matrix opperations\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "# Lets start with our matrix A\n",
    "A = scipy.array([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n",
    "# We can do QR decomposition with one function\n",
    "Q, R = scipy.linalg.qr(A)\n",
    "\n",
    "print(f\"A: {A}\")\n",
    "print(f\"Q: {Q}\")\n",
    "print(f\"R: {R}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Least-Squares Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{R}^{n*m}$ be an $n*m$ matrix, and $b \\in \\mathbb{R}^n$ be a vector. The system $Ax=b$ is consistant if there exists a set $K$ of n-tuple solutions $s$, such that $s = (s_1, \\ldots, s_n) \\in \\mathbb{R}^n$ satisfying $As = b$. If the solution set $K$ is empty, the system is inconsistant (i.e. if $K \\neq \\emptyset$, the system is consistent, if $K = \\emptyset$ the system is inconsistent). When $n=m$, we can use the inverse $A^{-1}$ to solve the system; however, when $n > m$, $A^{-1}$ does not exist. We can still solve this by finding the solution to the following least squares problem:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\min_{x \\in \\mathbb{R}^m} || Ax-b|| \\\\\n",
    "\\text{The solution to this satisfies:}\n",
    "\\\\\n",
    "A^TAx=A^Tb \\text{ and } Rx^*=Q^Tb, \\text{ where } x^* \\text{ is the solution that can be found using back substitution.}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of input data points $\\{(x_i,y_i)\\}^n_{i=1}$, where $x_i=(x_{i1},\\ldots,x_{id})^T$ is a column vector, we can find an affine function (a function thats composed of a linear function plus a constant) to fit the data (i.e. an affine function that is the line of best fit). This can be done by finding some coefficients $\\beta_j$ that minimizes the following:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\sum^n_{i=1}(y_i-\\hat{y_i})^2 \\\\\n",
    "\\hat{y_i} = \\beta_0+\\sum^d_{j=1}\\beta_jx_{ij}\n",
    "\\end{gather*}\n",
    "\n",
    "This can be viewed as the predicted values of the linear model with coefficients $\\beta_j$. The problem can be transformed to the following least-squares problem:\n",
    "\\begin{gather*}\n",
    "\\min_\\beta ||y-A\\beta||^2\n",
    "\\end{gather*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
