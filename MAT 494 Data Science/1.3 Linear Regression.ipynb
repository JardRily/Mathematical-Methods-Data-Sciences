{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR Decomposition can be used to solve the linear least squares problem and is the basis for the QR algorithm. The $Q$ is an orthogonal matrix, and $R$ is an upper trianglar matrix:\n",
    "\n",
    "\\begin{align}\n",
    "QQ^T = I \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; Q^T = Q^{-1} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n",
    "0 & a_{22} & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & a_{nn}\\\\\n",
    "\\end{bmatrix}\\;\\;\\;\\;\\;\\\\ \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR Decomposition follows the form $A = QR$, and in some cases is easier seen in the form $A^T = R^TQ^T $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[ 2 -6  3]\n",
      " [ 1 12 33]\n",
      " [ 2  9 -6]]\n",
      "Q:\n",
      "[[ 0.667 -0.667  0.333]\n",
      " [ 0.333  0.667  0.667]\n",
      " [ 0.667  0.333 -0.667]]\n",
      "R:\n",
      "[[ 3.  6.  9.]\n",
      " [-0. 15. 18.]\n",
      " [-0. -0. 27.]]\n"
     ]
    }
   ],
   "source": [
    "# SciPy is a much more powerful linear algebra tool in python, we'll be switching to that for functions\n",
    "# We will still use NumPy for array declaration\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "# QR decomposition can be done in one simple command\n",
    "A = np.array([[2, -6, 3], [1, 12, 33], [2, 9, -6]], dtype=int)\n",
    "Q, R = linalg.qr(A)\n",
    "Q, R = Q * -1, R * -1\n",
    "print(f\"A:\\n{A}\")\n",
    "print(f\"Q:\\n{Q}\")\n",
    "print(f\"R:\\n{R}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QR:\n",
      "[[ 2. -6.  3.]\n",
      " [ 1. 12. 33.]\n",
      " [ 2.  9. -6.]]\n",
      "QQT:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "AT:\n",
      "[[ 2  1  2]\n",
      " [-6 12  9]\n",
      " [ 3 33 -6]]\n",
      "ATcalc:\n",
      "[[ 2.  1.  2.]\n",
      " [-6. 12.  9.]\n",
      " [ 3. 33. -6.]]\n"
     ]
    }
   ],
   "source": [
    "# We can use these arrays and verify some properties above\n",
    "# A = QR\n",
    "QR = np.matmul(Q, R)\n",
    "# I = QQ^T\n",
    "QQT = np.matmul(Q, np.transpose(Q))\n",
    "# AT = RTQT\n",
    "AT = np.transpose(A)\n",
    "ATcalc = np.matmul(np.transpose(R), np.transpose(Q))\n",
    "print(f\"QR:\\n{QR}\")\n",
    "print(f\"QQT:\\n{QQT}\")\n",
    "print(f\"AT:\\n{AT}\")\n",
    "print(f\"ATcalc:\\n{ATcalc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Least-Squares Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{R}^{n*m}$ be an $n*m$ matrix, and $b \\in \\mathbb{R}^n$ be a vector. The system $Ax=b$ is consistant if there exists a set $K$ of n-tuple solutions $s$, such that $s = (s_1, \\ldots, s_n) \\in \\mathbb{R}^n$ satisfying $As = b$. If the solution set $K$ is empty, the system is inconsistant (i.e. if $K \\neq \\emptyset$, the system is consistent, if $K = \\emptyset$ the system is inconsistent). When $n=m$, we can use the inverse $A^{-1}$ to solve the system; however, when $n > m$, $A^{-1}$ does not exist. We can still solve this by finding the solution to the following least squares problem:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\min_{x \\in \\mathbb{R}^m} || Ax-b|| \\\\\n",
    "\\text{The solution to this satisfies:}\n",
    "\\\\\n",
    "A^TAx=A^Tb \\text{ and } Rx^*=Q^Tb, \\text{ where } x^* \\text{ is the solution that can be found using back substitution.}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of input data points $\\{(x_i,y_i)\\}^n_{i=1}$, where $x_i=(x_{i1},\\ldots,x_{id})^T$ is a column vector, we can find an affine function (a function thats composed of a linear function plus a constant) to fit the data (i.e. an affine function that is the line of best fit). This can be done by finding some coefficients $\\beta_j$ that minimizes the following:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\sum^n_{i=1}(y_i-\\hat{y_i})^2 \\\\\n",
    "\\hat{y_i} = \\beta_0+\\sum^d_{j=1}\\beta_jx_{ij}\n",
    "\\end{gather*}\n",
    "\n",
    "This can be viewed as the predicted values of the linear model with coefficients $\\beta_j$. The problem can be transformed to the following least-squares problem:\n",
    "\\begin{gather*}\n",
    "\\min_\\beta ||y-A\\beta||^2\n",
    "\\end{gather*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
