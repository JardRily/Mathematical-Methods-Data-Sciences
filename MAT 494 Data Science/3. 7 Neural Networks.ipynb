{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/3.%207%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT28BBhf_xNJ"
      },
      "source": [
        "# 3.7 Neural Networks\n",
        "\n",
        "Artificial neural networks are collections of connected layers of units or nodes to loosely model the neurons in a brain. This sections will illistrate the use of differentiation for training artificial neural networks to minimize cost functions.\n",
        "\n",
        "## 3.7.1 Mathematical Formulation\n",
        "\n",
        "A neural network has inputs on the left side, where the set of inputes can be thought of the set $\\{x^{(n)}_1, x^{(n)}_2, \\ldots, x^{(n)}_m\\}$, and a forecast output on the right $\\hat{y}$, which is modified by the activation function $\\sigma(z)$ chosen in advance: $\\hat{y} = \\sigma(z) = \\sigma (w_1 a_1 + w_2 a_2 + b)$. In neural networks, the weights, the $w_i$, and the bias, $b$, will be found numerically to best fit our forecast output with our given data.\n",
        "\n",
        "A general neural network may have any number of nodes. They demonstrate the inputs and outputs of neural networks. The input units receive various forms and structures of information based on an internal weighting system, and the neural network attempts to learn about the information presented to produce one output report. Specifically, it adjusts its weighted associations according to the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments, the training can be terminated based upon certain criteria. This is known as supervised learning.\n",
        "\n",
        "Now we formulate mathmatical notation for a neural netwrok: We can label each column in a neural network as a layer. In a general neural network, the rightmost layer will be called $Layer\\; l$, with the layer to the left of it being $Layer\\; l-1$. The values inputed on layer $l$ will be determined by the output of the values of layer $l-1$. Let \n",
        "\\begin{gather*}\n",
        "z^{(l)}_{j'} = \\sum_{j=1}^{J_{l-1}} w_{j,j'}^{(l)}, a_j^{(l-1)} + b_{j'}^{(l)}\n",
        "\\end{gather*}\n",
        "where $J_I$ means the number of nodes in layer $l$. For a given activation function, $\\sigma$, we end up with the following expression for the values in the next layer, $a_{j'}^{(l)} = \\sigma (z_{j'}^{(l)})$. In matrix form, $z^{(l)} = W^{(l)} a^{(l-1)} + b^{l}$, with the matrix $W^(l)$ containing all the multiplicative parameters (the weights $w^{(l)}_{j,j'}$ and $b^{(l)}$ is the bias). The bias is just the constant in the linear transformation: $a^{(l)} = \\sigma (z^{(l)}) = \\sigma (W^{(l)} a^{(l-1)} + b^{(l)})$.\n",
        "\n",
        "## 3.7.2 Activation Functions\n",
        "\n",
        "In neural networks, the activation function of a node abstracts the output of that node given an input or set of inputs for specific purposes (like classification). In biological neural networks, the activation function may represent an electrical signal, whether or not the neuron fires. We use $\\sigma$ to represnt the activation functions. It will be the same for all nodes in a layer: $a^{(l)} = \\sigma (z^{(l)}) = \\sigma (W^{(l)} a^{(l-1)} + b^{(l)})$.\n",
        "\n",
        "Here we discuss a number of activation functions.\n",
        "\n",
        "### 3.7.2.1 Step Function\n",
        "\n",
        "\\begin{gather*}\n",
        "\\sigma (x)=\n",
        "\\begin{cases}\n",
        "0 & x < 0 \\\\\n",
        "1 & x \\ge 0\n",
        "\\end{cases}\n",
        "\\end{gather*}\n",
        "This is also called the Heaviside step function, or the unit step funciton, often represents a signal that switches on at a specified time and stays switched on indefinie. The step function can be used for classification problems. \n",
        "\n",
        "### 3.7.2.2  Rectified Linear Units (ReLU) Function\n",
        "\n",
        "Positive linear/ReLU function is defined as $\\sigma(x) = \\max (0, x)$. It is one of the most commonly used activation functions. The signal either passes through untouched or dies completely. It is used to enable better training of deeper networks compared to the widely used activation functions. Rectified linear units, compared to sigmoid function or similar activation functions, allow faster and effective training of deep neural architectures on large and complex datasets.\n",
        "\n",
        "### 3.7.2.3 Sigmoid\n",
        "\n",
        "Sigmoid or logistic fucntion $\\sigma(x) = \\frac{1}{1+e^{-x}}$. The logisitc function finds applications in variety of fields, including biomathematics. The logistic sigmoid can be used in teh output layer for predicting probability.\n",
        "\n",
        "### 3.7.2.4 Softmax Function\n",
        "\n",
        "The softmax function converts a vector of number (an array of $K$ values ($z$)) into a vector of probabilities, where the probabiliities of each value are proportional to the relative scale of each value in the vector. It is thus a function that turns several numbers into quantities that can be perhaps interpreted as probabilites.\n",
        "\\begin{gather*}\n",
        "\\frac{e^{Z_K}}{\\sum^K_{k=1}e^{Z_k}}\n",
        "\\end{gather*}\n",
        "It is often used in the final output layer of a neural network, expecially with classification problems.\n",
        "\n",
        "## 3.7.3 Cost Function\n",
        "\n",
        "In practice, we can use the least squares for a cost function. Since we will have a set of independent input data $y^n$ (from the training dataset) and corresponding output data $\\hat{y^n}$ or the forecast output. $k$ is the $k$-th node of the output. We define the cost function as\n",
        "\\begin{gather*}\n",
        "J=\\frac{1}{2}\\sum^N_{n=1} \\sum^K_{K=1} (\\hat{y^{(n)})k}-y^{(n)}_k)^2\n",
        "\\end{gather*}\n",
        "\n",
        "For classification problems where only one output, the cost fucntion commonly used for such an output is similar to logistic regression. And this is, for a binary classification $(y^{(n)} = 0, 1)$, the cost function is\n",
        "\\begin{gather*}\n",
        "J=-\\sum^N_{n=1} (y^{(n)} ln(\\hat{y^{(n)}}) + (1-y^{(n)}) ln(1-\\hat{y^{(n)}}))\n",
        "\\end{gather*}\n",
        "This is related to the cross entropy function.\n",
        "\n",
        "## 3.7.4 Backpropagation\n",
        "\n",
        "Back-propogation is the essence of neural network training. It is the practice of fine-tuning the weights of a neral network based on the error rate obtained in the previous iteration. Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization. We want to minimize the cost fucntion, $J$, with respect to the parameters, the components of $W$ and $b$. To do that using gradient descent, we are going to need the derivatives of $J$ with respect to each of those parameters. Here we focus the layer $l$ and node $j'$ and node $j$ from layer $l-1$.\n",
        "\\begin{gather*}\n",
        "\\frac{\\partial J}{\\partial w^{(l)}_{j,j'}}\\;\\text{and}\\;\\frac{\\partial J}{\\partial b_{j'}^{(l)}}\n",
        "\\end{gather*}\n",
        "\n",
        "We introduce the quantity \n",
        "\\begin{gather*}\n",
        "\\delta_{j'}^{(l)}= \\frac{\\partial J}{z_{j'}^{(l)}}\n",
        "\\end{gather*}\n",
        "From the chain rule we have:\n",
        "\\begin{gather*}\n",
        "\\delta_{j}^{(l-1)}=\\frac{\\partial J}{z_{j}^{(l-1)}}=\\sum_{j'} \\frac{\\partial J}{z_{j'}^{(l)}} \\frac{\\partial z_{j'}^{(l)}}{z_{j}^{(l-1)}}\n",
        "\\end{gather*}\n",
        "\n",
        "It follows that\n",
        "\\begin{gather*} \n",
        "z_{j'}^{(l)} = \\sum_{j_k} w^{(l)}_{j_k,j'} a^{(l-1)}_{j'} = \\sum_{j_k} w^{(l)}_{j_k,j'}, \\sigma (z^{(l-1)}_{j_k}) + b^{(l)}_{j'}\n",
        "\\end{gather*}\n",
        "\n",
        "In addition,\n",
        "\\begin{gather*} \n",
        "\\delta_{j}^{(l-1)} = \\frac{dg^{(l-1)}}{dz} \\bigg|_{z_{j}^{(l-1)}} \\sum_{j'} \\frac{\\partial J}{\\partial z_{j'}^{(l)}} w_{j,j'}^{(l)} = \\frac{dg^{(l-1)}}{dz}\\bigg|_{z_{j}^{(l-1)}} \\sum_{j'} {\\delta}_{j'}^{(l)} w_{j,j'}^{(l)}\n",
        "\\end{gather*}\n",
        "\n",
        "As a result, we can find that the $\\delta$'s in a layer if we know the $\\delta$'s in all layers to the right. In summer, we have \n",
        "\\begin{gather*}\n",
        "\\frac{\\partial J}{\\partial w_{j,j'}^{(l)}} = \\frac{\\partial J}{\\partial z_{j'}^{(l)}} \\frac{\\partial z_{j'}^{(l)}}{\\partial w_{j, j'}^{(l)}} = {\\delta}_{j'}^{(l)} a_{j}^{(l-1)}\n",
        "\\end{gather*}\n",
        "\n",
        "Now the derivatives of the cost function $J$, to the $w$'s, can be written in terms of the $\\delta$'s, which in turn are backpropagated from the network layers that are just to the right, one nearer the output. And the derivative of the cost function to the bias, $b$, is simply:\n",
        "\\begin{gather*}\n",
        "\\frac{\\partial J}{\\partial b_{j'}^{(l)}} = {\\delta}_{j'}^{(l)}\n",
        "\\end{gather*}  \n",
        "\n",
        "It is clear that the derivatives of $J$ depend on which activation function we use. If it is ReLU, then the derivative is either zero or one. If we use the logistic function, then we find that ${\\sigma}' (z) = \\sigma (1 - \\sigma)$.\n",
        "\n",
        "## 3.7.5 Backpropogation Algorithm\n",
        "\n",
        "From above analysis, we can easily derive the backpropogation algorithm as follow: First we initialze weights and biases, typically at random. Then pick input data and input the vector $x$ into the left side of the network, and calculate all the $z_s, a_s, etc.$. Finally, calculate the output $\\hat{y}$. We can now update hte parameters by the (stocahstic) gradient descent. Repeat the process until the desired accuracy is reached. For example, if using the quadratic cost function in one dimension, then \n",
        "\\begin{gather*}\n",
        "{\\delta}^(L) = \\frac{dg^{(L)}}{dz} \\bigg|_{z_j^{(L)}} (\\hat{y} - y)\n",
        "\\end{gather*}\n",
        "Continue to the left\n",
        "\\begin{gather*}\n",
        "{\\delta}_{j}^{(l-1)}=\\frac{dg^{(l-1)}}{dz} \\bigg|_{z_{j}^{(l-1)}} \\sum_j {\\delta}_{j'}^{l} w_{j, j'}^{(l)}\n",
        "\\end{gather*}\n",
        "\n",
        "Then update the weights and biases using the following formulas.\n",
        "\\begin{gather*}\n",
        "New\\;w^{(l)}_{j,j'} = Old\\; w^{(l)}_{j,j'} - \\beta \\frac{\\partial J}{\\partial w^{(l)}_{j,j'}} = Old\\;w^{(l)}_{j,j'}-\\beta {\\delta}^{(l)}_{j'} a^{(l-1)}_{j}\n",
        "\\end{gather*}\n",
        "and\n",
        "\\begin{gather*}\n",
        "New\\;b^{(l)}_{j'} = Old\\; b^{(l)}_{j'} - \\beta \\frac{\\partial J}{\\partial b^{(l)}_{j'}} = Old\\;b^{(l)}_{j'}-\\beta {\\delta}^{(l)}_{j'}\n",
        "\\end{gather*}"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}