{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/3_3_Unconstrained_Optimizatoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3kV2kybfnQo"
      },
      "source": [
        "# 3.3 Unconstrained Optimization\n",
        "\n",
        "## 3.3.1 Necessary and Sufficent Conditions of Local Minimizers\n",
        "\n",
        "We will be looking at unconstrained optimization of the form:\n",
        "\\begin{gather*}\n",
        "\\min_{x\\in\\mathbb{R}^d} f(x)\n",
        "\\end{gather*}\n",
        "where $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$. In this section we are going to use many forms, ideally to find a global minimizer to the optimization problem above.\n",
        "\n",
        "Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R} $. The point $x^* \\in \\mathbb{R}^d $ is a global minimizer of $f$ over $\\mathbb{R}^d$ if $f(x) \\ge f(x^*), \\forall x \\in \\mathbb{R}^d$. Often it is difficult to find a global minimizer unless some special structure is present, which is why we will be introducing weaker notations of solutions. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$. The point $x^* \\in \\mathbb{R}^d$ is a local minimizer of $f$ over $\\mathbb{R}^d$ if there is a $\\delta > 0$ such that: $f(x) \\ge f(x^*), \\forall x \\in B_\\delta (x^*) \\; \\backslash \\; \\{x^*\\}$. If the inequality is strict, we say that $x^*$ is a strict local minimizer.\n",
        "\n",
        "$x^*$ is a local minimizer if there is an \"open ball\" around $x^*$ where it attains the minimum value. We will characterize local minimizers in terms of the gradient and Hessian of the function. We first need to define what a descent direction is, which generalizes the case when the derivative of a one dimensional fucntion is negative. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$. A vector $v$ is a descent direction for $f$ at $x_0$ if there is an $\\alpha^* > 0$ such that $f(x_0 + \\alpha v) < f(x_0), \\forall \\alpha \\in (0, \\alpha^*)$. In the continuously differentiable case, the directional derivative gives a criterion for descent directions. \n",
        "\n",
        "Let $f : \\mathbb{R}^d \\to \\mathbb{R} $ be continuously differentiable at $x_0$. A vector $v$ is a descent direction for $f$ at $x_0$ if $\\partial \\frac{f(x_0)}{\\partial v} = \\nabla f(x_0)^T v < 0$, that is the directional derivative of $f$ at $x_0$ in the direction $v$ is negative. \n",
        "\n",
        "Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable at $x_0$ and assume that $\\nabla f(x_0) \\neq 0$. Then, $f$ has a descent direction at $x_0$.\n",
        "\n",
        "Let $f : \\mathbb{R}^d \\to \\mathbb{R} $ be continuously differentiable on $\\mathbb{R}^d$. If $x_0$ is a local minimizer, then $\\nabla f(x_0) = 0$.\n",
        "\n",
        "A square symmetric $d \\times  d$ matrix $H$ is a positive semi-definite (PSD) if $x^T Hx \\ge 0$ for any $x \\in \\mathbb{R}^d$. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be twice continuously differentiable on $\\mathbb{R}$. If $x_0$ isa local minimizer, then $H_f(x_0)$ is PSD. \n",
        "\n",
        "### 3.3.1.1 Sufficient Conditions for Local Minimizers\n",
        "\n",
        "As in the one dimensional case, the necessary conditions int he previous subsection are not in general sufficient. We can state the following theorum which give a sufficient conditions for local minimizers. Let $f : \\mathbb{R}^d \\to \\mathbb{R}$ be twice continuously differentiable on $\\mathbb{R}^d$. If $\\nabla f(x_0)=0$ and $H_f (x_0)$ is positive definite, then $x_0$ is a strict local minimizer.\n",
        "\n",
        "## 3.3.2 Convexity and Global Minimizers\n",
        "\n",
        "A real valued function is called convex if the line segment between any two points on the graph of the function lies above the graph between the two points."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}