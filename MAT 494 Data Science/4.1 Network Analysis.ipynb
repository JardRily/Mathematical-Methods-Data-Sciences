{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/MAT%20494%20Data%20Science/4.1%20Network%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d64If5h6HrTs"
      },
      "source": [
        "# 4.1 Network Analysis\n",
        "\n",
        "Network analysis is key for modern data analysis since most data is stored in network-structures. A network structure is something that analyzes correlations between variables and creates correlation networks (which is something often used for data mining). These networks can be modeled graphically, where individuals in a network are nodes, and edges connect nodes by the relationship that characterizes the network. They often show community structures, with clusters of nodes forming that show specific communal groups. In practical use, it's more important to look at how communitiesform and are related to each other, then how arbitrary nodes are connected. \n",
        "\n",
        "# 4.1.1 Graph Models\n",
        "\n",
        "In this section, we first review some common notation of graphs. Any graph consisting of both a set of objects (nodes) and the connections (edges) can be mathematically denoted as a pair $G(V,E)$, where $V=\\{v_1,v_2,\\ldots ,v_n\\}$ represents the set of nodes, and $E=\\{e_1,e_2,\\ldots, e_m\\}$ represents the set of edges. Edges are also represented by their endpoints (nodes), so $e(v_1, v_2)$ or $(v_1,v_2)$ defines an edge between nodes $v_1$ and $v_2$. When edges are undirected, nodes are connected both ways and are called undirected edges, which form an undirected graph. Graphs that have only directed edges are called directed graphs. A graph with both undirected and directed edges is called a mixed graph. \n",
        "\n",
        "A sequence of edge where nodes and edges are distinct, $e_1(v_1,v_2), e_2(v_2, v_3), e_3(v_3,v_4)$ is called a path. A closed path is called a cycles. The length of a path or cycle is the number of edges traversed in the path or cycle. In a directed graph, we count only directed paths (since traversal is one way). For a connected graph, multiple paths can exist between paits of nodes. Often, we are interested in the path that has the shortest length.The concept of the neighborhood of a node $v_i$ is the set of nodes that are within $n$ hops distance from the node $v_i$.\n",
        "\n",
        "The degree of a node in a graph, which is the nubmer of edges connected to the node, plays a significant role in the study of graphs. For a directed graph, there are two types of degree: 1) in-degrees (edges towards node) and 2) out-degrees (edges awawy from the node). In a network, the nodes with the most connections possess the greatest degree of centrality. Degere centrality measures relative levels of importance. We often regard people with many interpersonal connections to be more important than those with few. In-degree cetrality described the gregariuosness of the node (ie for social media, the number of friends a node has)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhQFItjfHrTv"
      },
      "source": [
        "## 4.1.2 Laplacian Matrix\n",
        "\n",
        "A graph with $n$ nodes can be represented by a $n \\times n$ adjacency matrix. A value of 1 at row $i$, column $j$ in the adjacency matrix indicates a connection between $v_i$ and $v_j$, and a value of 0 denotes no connection between the two nodes. In directed graphs, we can have two edges between two nodes (both going opposite directions); in undirected graphs there can only be one. As a result, the adjaceny matrix for directed graphs is not symmetric, while the adjacency matrix for an undirected graph is ($A=A^T$).\n",
        "\n",
        "Consider a weighted graph $G=(V,E)$, with $n$ vertices and $m$ eges each wth weights $E_{i,j}$ connecting nodes $i$ and $j$. The adjacency of matrix $M$ of a graph is defined by $M_{ij}=E_{ij}$ if there is an edge $\\{i,j\\}$ and $M_{ij} = 0$ if not. The Laplacian Matrix $L$ of $G$ is an $n \\times n$ symmetric matrix, with one row and column for each vertex, such that\n",
        "\\begin{gather*}\n",
        "L_{ij}=\n",
        "\\begin{cases}\n",
        "\\sum_k E_{ik} & i=j \\\\\n",
        "-E_{ij} & i \\neq j\\; \\text{and}\\; v_i\\; \\text{is adjacent to}\\; v_j \\\\\n",
        "0 & otherwise \n",
        "\\end{cases}\n",
        "\\end{gather*}\n",
        "\n",
        "In addition, a $n \\times m$ incidence matrix of $G$, denoted by $I_G$, has one row per vertex and one column per edge. The column corresponding to edge $\\{i,j\\}$ of $I_G$ is zero, except the $i-th$ and $j-th$ entries, which are $\\sqrt{E_{ij}}$ and $-\\sqrt{E_{ij}}$ respectivly. Assume a graph of three nodes in a row, connected by unweighted, undireceted connections. Then a matrix for this graph would be:\n",
        "\\begin{gather*}A=\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 0 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "0 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "\\end{gather*}\n",
        "We wish to use a vector $c$ to describe the significance of each node by multiplying the corresponding degree to each node: $\\lambda  c = Ac$, which is $(A-\\lambda I)c=0$. Assuming that $c=[u_1\\;u_2\\;u_3]^T$,\n",
        "\\begin{gather*}det(A-\\lambda I)=\n",
        "\\begin{bmatrix}\n",
        "0-\\lambda & 1 & 0 \\\\\n",
        "1 & 0-\\lambda & 1 \\\\\n",
        "0 & 1 & 0-\\lambda\n",
        "\\end{bmatrix}\n",
        "=0\n",
        "\\end{gather*}\n",
        "Therefore, the eigen values are $(-\\sqrt{2}, 0, \\sqrt{2})$. The largest eigenvalue is $\\sqrt{2}$, and that is what we choose to compute. The computed corresponding eigenvector is:\n",
        "\n",
        "\\begin{gather*}\n",
        "\\begin{bmatrix}\n",
        "0-\\sqrt{2} & 1 & 0 \\\\\n",
        "1 & 0-\\sqrt{2} & 1 \\\\\n",
        "0 & 1 & 0-\\sqrt{2}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "u_1 \\\\ u_2 \\\\ u_3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0 \\\\ 0 \\\\ 0\n",
        "\\end{bmatrix}\n",
        "\\end{gather*}\n",
        "\n",
        "Asuuming the $c$ vector has a norm of 1, the solution is \n",
        "\\begin{gather*}\n",
        "C_e = \n",
        "\\begin{bmatrix}\n",
        "u_1 \\\\ u_2 \\\\ u_3\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{2} \\\\ \\frac{\\sqrt{2}}{2} \\\\ \\frac{1}{2}\n",
        "\\end{bmatrix}\n",
        "\\end{gather*}\n",
        "which states that node $v_2 is the most central node, while nodes $v_1$ and $v_3$ posses equal centrality values.\n",
        "\n",
        "Some properties of the Laplacian Matrix $L$:\n",
        "1) $L=D-M$, where $M$ is the adjacency matrix, and $D$ is the diagonal degree matrix with $D_{ii}={\\Sigma}_k E_{ik}$.\n",
        "2) $L=I_G I_G^T$\n",
        "3) $L$ is symmetric positive semi-definite. All eigenvalues of $L$ are real and non-negative, and $L$ has a full set of $n$ real and orthogonal eigenvectors.\n",
        "4) Let $e=[1,\\ldots,1]^T. Then $Le=0$. Thus, 0 is the smallest eigenvalue and $e$ is the corresponding eigenvector.\n",
        "5) If the graph $G$ has $c$ connected components, then $L$ has $c$ eigenvalues that is 0.\n",
        "6) For any vector $x$, $x^T Lx=\\Sigma_{\\{i,j\\}\\in E} E_{ij} (x_i - x_j)^2.\n",
        "7) For any vector $x$ and scalars $alpha, \\beta, (\\alpha x + \\beta e) = \\alpha^2 x^T Lx\n",
        "8) The problem $\\min_{x\\neq 0} x^T Lx$ ,subject to, $x^T x = 1, x^T e=0$ is solved when $x$ is the eigenvector corresponding to the second smallest eigenvalues (Fiedler vector) $\\lambda_2$ of the eigenvalues problem $Lx=\\lambda x$. \n",
        "\n",
        "The Courant-Fischer Theorem: Let $A$ be $n \\times n$ symmetric matrix $A$ with orthogonal diagonalization $A=PDP^{-1}$. The columns of $P$ are orthonormal eigenvectors $v_1, \\ldots, v_n$ of $A$. Assume that the diagonal of $D$ are arranged so that $\\lambda_1 \\le \\lambda_2 \\ldots \\le \\lambda_n$. Let $S_k$ be the span of $v_1, \\ldots, v_k$ and $S^{\\bot}_k $ denote the orthogonal complement of $S_k$. Then \\min_{x\\neq 0, x \\in S^{\\bot}_{k-1} \\frac{x^T Ax}{x^T x} = \\lambda_k$. When $k=2, S^{\\bot}_1$ is all $x$ such that $x\\bot v_1$, or $v_1^T \\cdot x=0$, which implies the following result. \n",
        "\n",
        "Let $A$ be $n \\times n$ symmetric matrix $A$ with orthogonal diagonalization $A=PDP^{-1}$. The columns of $P$ are orthonormal eigenvectors $v_1, \\ldots, v_n$ of $A$. Assume that the diagonal of $D$ are arranged so that $\\lambda_1 \\le \\lambda_2 \\ldots \\le \\lambda_n$. Then $\\min_{x\\neq 0, x^T v_1 = 0} \\frac{x^T Ax}{x^T x} = \\lambda_2$."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}