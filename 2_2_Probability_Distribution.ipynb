{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JardRily/Mathematical-Methods-Data-Sciences/blob/main/2_2_Probability_Distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abq0fjwsnmEu"
      },
      "source": [
        "# 2.2 Probability Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbsB3cCynmEv"
      },
      "source": [
        "A probability distribution is a function that gives the probabilities of certain events occuring for a given experiment. Their are both Discrete Probability Distributions and Continous Probability Distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYF3YBzCnmEv"
      },
      "source": [
        "## 2.2.1 Probability Axioms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdT5fXzHnmEv"
      },
      "source": [
        "Experiment - an activity who's process whose outcome can be uncertain. This can be something as complex as measuring the occurance of a genome in a species of insects, or as simple as flipping a coin.\n",
        "\n",
        "Sample Space - Denoted as $S$, the set of all possible outcomes of an experiment. Can be infinite or finite.\n",
        "\n",
        "Event - Any subset of $S$. This can be thought of as the collection of outcomes we are studying.\n",
        "\n",
        "Probability Distribution Function - Given an experiment with sample space $S$, each event $A$ has a specific function that represents the probability of that event occuring denoted as $P(A)$. The probability asignments must follow these rules:\n",
        "- For and event $A, 1 \\ge P(A) \\ge 0$\n",
        "- $P(S) = 1$\n",
        "- If $A_1, A_2, A_3,...$ is an infinite collection of disjoint events, then \n",
        "$P(A_1 \\cup A_2 \\cup P(A_3) \\cup \\ldots) = \\sum_{i=1}^\\infty P(A')$\n",
        "- For any event $A, P(A) + P(A') = 1$, from which $P(A) = 1 - P(A')$\n",
        "- When events $A$ and $B$ are mutually exclusive, $P(A \\cup B)=P(A) + P(B)$\n",
        "- For any two events $A$ and $B$, $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwlLJUNNnmEw"
      },
      "source": [
        "#### Example 1\n",
        "Lets look at a common experiment, flipping a coin. We know that when we flip a coin, there are 2 possible outcomes, thus are sample space is: $S = \\{heads,\\;tails\\}$. We can see from this that there is a $\\frac{1}{2}$ chance that we will get the outcome $heads$ when we flip a coin, and a $\\frac{1}{2}$ chance we will get outcome $tails$ when we flip a coin. We can better denote this as $P(heads) = \\frac{1}{2}$ and $P(tails) = \\frac{1}{2}$. We can double check that this is correct by verifying some of the rules: $P(S) = P(heads) + P(tails) = 1$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOiMtPlVnmEw"
      },
      "source": [
        "#### Example 2\n",
        "Now consider the act of rolling a dice. Our sample space $S = \\{1, \\;2, \\;3, \\;4, \\;5, \\;6\\}$. We know from real life experience that the probability of rolling any number $A$ in the set of $S$ will be $\\frac{1}{6}$ (ie $P(3) = \\frac{1}{6}$). If we add up all the probabilities, we see $P(S) = P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipxCBCjBnmEw"
      },
      "source": [
        "Notice how in both examples, there were $N$ number of outcomes, and each outcome had exactly $\\frac{1}{N}$ probability of happening. In these examples, we can denote the probability of any event $A$ occuring as $P(A) = \\frac{N(A)}{N}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyFjkDd_nmEw"
      },
      "source": [
        "## 2.2.2 Conditional Probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UmIGKuwnmEx"
      },
      "source": [
        "This can be described as the likelihood of an outcome occuring based on the occurence of a previous outcome. This can be expressed in a ratio $P(A | B) = \\frac{P(A \\cap B)}{P(B)}$. This also means that $P(A \\cap B) = P(A | B) \\cdot P(B)$. From this rule, we can gather a definition of inependence. $A$ and $B$ are independent events if $P(A | B) = P(A)$ or $P(A \\cap B) = P(A) \\cdot P(B)$. This rule can be extended into a more general rule: Events $A_1, \\ldots, A_n$ are mutually independent if for every $k = 1, 2, \\ldots, n$ and for every subset of indices $i_1, i_2, ..., i_k$: $P(A_{i1} \\cap A_{i2} \\cap \\ldots \\cap A_{ik}) = P(A_{i1}) \\cdot P(A_{i2}) \\cdot P(A_{ik})$ (where $P(A_{ik})$ represents the probability of a subset of $S$ occures)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-PgMM6RnmEx"
      },
      "source": [
        "## 2.2.3 Discrete Random Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4k3B-SGnmEx"
      },
      "source": [
        "A random variable is a measurable function whos values depend on outcomes of a random phenomenon. For a given sample space $S$, a random variable is any rule that associates a number with each outcome in $S$ (ie the random variable is a function whos domain is $S$, and whose range is $\\mathbb{R}$).\n",
        "\n",
        "A discrete random variable is a random variable whose possible values are a finite set, or can be listed in an infinite sequence. \n",
        "\n",
        "A random variable is continuous if: \n",
        "- Its set of possible values consit of all number in a single interval on the number line.\n",
        "- $P(X = c) = 0$ for any possible value individual $c$.\n",
        "\n",
        "Probability Mass Function - a funciton that gives the probability that a discrete random variable is exactly equal to some value. The pmf of a probability distribution specifies the probability of observing that value when the experiment is performed. The probability distributio or pmf of a discrete random variable is defined for every number x by $p(x) = P(X=x)=P(all\\;s\\in S:X(s)=x)$\n",
        "\n",
        "Cumulitive Distribution Function - $F(x)$ of a discrete random variable $X$ with pmf $p(x)$ is defined for every number $x$ by $F(x) = P(X \\le x) = \\Sigma_{y:y\\le x}\\;p(y)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gHfBrb6nmEy"
      },
      "source": [
        "### 2.2.3.1 Bernoulli Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJgezWsInmEy"
      },
      "source": [
        "Any random variable whose only possible values are 0 and 1 are called a Bernoulli random variable. The binomial random variable $X$ associated with independent Bernoulli experiment consisting of $n$ trials is defined as $X =\\;the\\;number\\;of\\;1's\\;among\\;the\\;n\\;trials$. The probability of success is constant $p$ from trial to trial. The pmf of $X$ has the form \n",
        "\\begin{gather*}\n",
        "b(x;n, p) = \n",
        "\\begin{cases}\n",
        "{n \\choose x}p^x(1-p)^{n-x} & x=0, 1, 2, 3,\\ldots,n\\\\\n",
        "0 & otherwise\n",
        "\\end{cases}\n",
        "\\end{gather*}\n",
        "The cdf of $X$ has the form\n",
        "\\begin{gather*}\n",
        "B(x;n, p) = P(X \\le x)= \\sum_{y \\le x}b(x;n, p) = \\sum_{y=0}^x {n \\choose y}p^x(1-p)^{n-x}\n",
        "\\end{gather*}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPphAjg2nmEy"
      },
      "source": [
        "### 2.2.3.2 Poisson Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW9iqe8anmEy"
      },
      "source": [
        "The Possion Distribution is a discrete probability distribution that describes the probability of a given number of events occurring in an interval of time or space. We use this distribution if we know a constant mean rate that is independent of the time since the last event. A discrete random variable $X$ has Possion distribution with parameter $\\mu$ if the pmf of $X$ is \n",
        "\\begin{gather*}\n",
        "p(x;\\mu) = \\frac{e^{-\\mu}\\mu^x}{x!}, x=0, 1, 2, 3, \\ldots\n",
        "\\end{gather*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9JImL5nmEy"
      },
      "source": [
        "### 2.2.3.3 The Expected Value of Variance of X\n",
        "The expected value of a random variable $X$ is a generalization of the weighted average, and is intuitively the arithmetic mean of a large number of independent realizations of $X$. Let $X$ be a discrete random variable with set of possible values $D$ and pmf $p(x)$. The expected values or mean value of $X$, denoted by $E(X), \\mu_x,$ or $\\mu$ is $E(X) = \\mu_x = \\Sigma_{x\\in D} \\;x \\cdot p(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnpZU6oVnmEy"
      },
      "source": [
        "#### Example 1\n",
        "Let $X=1$ be the Bernoulli random variable with pmf $p(1) = p, p(0) = 1- p$ and, from which $E(X) = 0 \\times p(0) + 1 \\times p(1)=p$. That is, the expected value of $X$ is just the probability that $X$ takes on the value $1$. If the random variable $X$ has a set of possible values $D$ and pmf $p(x)$, then the expected value of any function $h(X)$, denoted by $E[h(X)]$ or $\\mu_{h(X)}$ is computed by \n",
        "\\begin{gather*}\n",
        "E[h(X)] = \\sum_D h(x) \\cdot p(x)\\\\\n",
        "E(aX + b) = a \\cdot E(X) + b\n",
        "\\end{gather*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbIrybYWnmEz"
      },
      "source": [
        "Variances measure how far a set of numbers is spread out from their average value. Let $X$ have pmf $p(x)$ and expected value $\\mu$. Then the variance of $X$, denoted by $V(X)$ or $\\sigma_X^2, or just \\sigma^2, is \n",
        "\\begin{gather*}\n",
        "V(X) = \\sum_D(x-\\mu)^2 \\cdot p(x) = E[(X-\\mu)^2]\n",
        "\\end{gather*}\n",
        "\n",
        "The standard deviation of X is $\\sigma_X = \\sqrt{\\sigma_X^2}$\n",
        "\n",
        "There are two distributions whos expected values and variances are important to know:\n",
        "- If $X$ is a binomial random variable with parameters $n, p$, then $E(X) = np, V(X) = np(1-p), \\sigma_X = \\sqrt{np(1-p)}$\n",
        "- If $X$ is a Poisson distribution with parameter $\\mu$, then $E(X) = \\mu, V(X) = \\mu$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLJV-b_GnmEz"
      },
      "source": [
        "## 2.2.4 Continuous Random Variables\n",
        "A random variable X is continous if all possible values comprise either a singla interval on the number line or a union of disjoint intervals. Let X be a continous random variable. Then a probability distribution or probability density function of X is a function f(x) such that for any two nubers $a$ and $b$ with $a \\le b$:\n",
        "\\begin{gather*}\n",
        "P(a \\le X \\le b) = \\int_a^b f(x)dx\n",
        "\\end{gather*}  \n",
        "\n",
        "That is the probability that $X$ takes on a values in the interval $[a, b]$ is the area under the probability funciton $f(x)$ in this interval. $f(x)$ must satisfythe following two conditions:\n",
        "- $f(x) \\ge 0$ for all $x$\n",
        "- $\\int_{-\\infty}^{\\infty} f(x)dx = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjWSCTywnmEz"
      },
      "source": [
        "### 2.2.4.1 Expected Values and Variances\n",
        "The expected or mean value of a continuous random variable $X$ with pdf $f(x)$ is $\\mu_X = E(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) dx$. The variance of a continuous random variable $X$ with pdf $f(x)$ and the variance is $\\mu$ is $\\sigma_X^2 = V(X) = \\int_{-\\infty}^{\\infty} (x- \\mu)^2 \\cdot f(x)dx = E[(X - \\mu)^2]$. The standard deviation of $X$ is $\\sigma_X = \\sqrt{V(X)}$. The Expected values and variance have the following properties:\n",
        "- If X is a continuous random variable with pdf $f(x)$ and $h(X)$ is any function of $X$, then $E[h(X)] = \\mu_{h(X)} = \\int_{-\\infty}^{\\infty} h(X) \\cdot f(X)dx$\n",
        "- $V(X) = E(X^2) - [E(X)]^2$\n",
        "\n",
        "$X$ is said to have an exponential distribution with parameter $\\lambda (\\lambda > 0) $ if the pdf of $X$ is \n",
        "\\begin{gather*}\n",
        "f(x;h)=\n",
        "\\begin{cases}\n",
        "\\lambda e^{-\\lambda x} & x \\ge 0\\\\\n",
        "0 & otherwise\n",
        "\\end{cases}\n",
        "\\end{gather*}\n",
        "\n",
        "The expected value of an exponentially distributed random variable $X$ is $E(X) = \\int_{0}^{\\infty} x\\lambda e^{-\\lambda x}dx$.\n",
        "Obtaining this expected value neccessitates doing an integration by parts. The variance of $X$ can be computed using the fact that $V(X) = E(X^2) - [E(X)]^2$. The determination of $E(X^2)$ requires integrating by parts twice in succession. The results of these are $\\mu = \\frac{1}{\\lambda}\\; \\sigma^2 = \\frac{1}{\\lambda^2}$. Both the mean and the standard deviation of the exponential distribution equal $\\frac{1}{\\lambda}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AjAQrF9nmEz"
      },
      "source": [
        "### 2.2.4.2 The Normal Distribution\n",
        "\n",
        "Normal distributions are often used to represent real-valued random variables wohse distributions are not known. A continuous random variable $X$ is said to have a normal distribution with parameters $\\mu$ and $\\sigma$ (or $\\mu$ and $\\sigma^2$), where $-\\infty < \\mu < \\infty$ and $0 < \\sigma$, if the pdf of $X$ is $f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$ where $-\\infty < x < \\infty$.\n",
        "\n",
        "The computation of $P(a \\le X \\le b)$ when $X$ is a normal random variable with parameters $\\mu$ and $\\sigma$ requires evaluating\n",
        "\\begin{gather*}\n",
        "\\int_a^b \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} dx\n",
        "\\end{gather*}\n",
        "\n",
        "The normal distribution with parameter values $\\mu = 0$ and $\\sigma = 1$ is called the standard normal distribution. A random variable having a standard normal distribution is called a standard normal random variable and will be denoted by $Z$. The pdf of $Z$ is $f(z; 0, 1) = \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-z^2}{2}}$ where $-\\infty < x < \\infty$. the function of f(z; 0, 1) is known as the normal curve (or z curve). its inflection points are at 1 and -1. The cdf of $Z$ is $P(Z \\le z) = \\int_{-\\infty}^Z f(y; 0, 1) dy$, which is denoted by $\\Phi(z)$.\n",
        "\n",
        "A normal distribution, $X ~ N(\\mu, \\sigma^2)$ can be converted to the standardized variables $Z = \\frac{X-\\mu}{\\sigma}$. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a95447f1ef4a7eeb81744d913c0526f7f23f9aec6d814ce91d70cd9b0479878c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}